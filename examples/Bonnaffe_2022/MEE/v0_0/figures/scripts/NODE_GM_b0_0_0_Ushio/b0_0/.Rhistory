# "Chaenogobius.gulosus", #
"Pterogobius.zonoleucus",
"Tridentiger.trigonocephalus",
# "Siganus.fuscescens", #
"Sphyraena.pinguis", #
"Rudarius.ercodes"
)
TS = TS[selected_time_steps,]
TS = TS[,selected_columns]
## set 0s to small value
for(i in 2:ncol(TS)){TS[,i][which(TS[,i]<0.005)] = 0.005}
## normalise time series
TS[,-1] = apply(TS[,-1],2,function(x)(x-min(x))/(max(x)-min(x))*10)
## visualise time series
par(mfrow=c(3,4))
for(i in 2:ncol(TS))
{
plot(TS[,1],TS[,i],type="l",xlab="Time step",ylab="Count",bty="n",main=colnames(TS)[i])
}
par(mfrow=c(1,1))
## make output directory
pathToOut = "out"
system(paste("mkdir",pathToOut))
pdf(paste(pathToOut,"fig_time_series.pdf",sep=""))
par(mfrow=c(3,4))
for(i in 2:ncol(TS))
{
plot(TS[,1],TS[,i],type="l",xlab="Time step",ylab="Count",bty="n",main=colnames(TS)[i])
}
par(mfrow=c(1,1))
dev.off()
## parameters of observation model
N       = ncol(TS) - 1
K_o     = 10                  # number of ensemble elements
W_o     = rep(30,N)          # number of neurons in observation model, by default a single layer perceptron (equivalent to number of elements in Fourier series)
N_o     = W_o*3              # total number of parameters in observation model
rho     = 1                  # proportion of best samples to reject (to refine quality of fit if necessary)
alpha_i = 1                  # upsampling interpolation factor (2 double the number of points in interpolated time series)
alpha_i
## train observation model
model_o     = trainModel_o(TS,alpha_i,N_o,K_o,rho)
## load data
TS = read.table("data/TS.csv",sep=",",header=T)
## extract column of interest
selected_time_steps = 50:150
selected_columns  = c(
"time_step",
# "surf.t",
"bot.t",
"Aurelia.sp",
# "Engraulis.japonicus", #
# "Plotosus.lineatus", #
"Sebastes.inermis",
"Trachurus.japonicus",
"Girella.punctata",
"Pseudolabrus.sieboldi",
"Halichoeres.poecilopterus",
"Halichoeres.tenuispinnis",
# "Chaenogobius.gulosus", #
"Pterogobius.zonoleucus",
"Tridentiger.trigonocephalus",
# "Siganus.fuscescens", #
"Sphyraena.pinguis", #
"Rudarius.ercodes"
)
TS = TS[selected_time_steps,]
TS = TS[,selected_columns]
## normalise time series
TS[,-1] = apply(TS[,-1],2,function(x)(x-min(x))/(max(x)-min(x))*10)
## set 0s to small value
for(i in 2:ncol(TS)){TS[,i][which(TS[,i]<0.005)] = 0.005}
## make output directory
pathToOut = "out"
system(paste("mkdir",pathToOut))
## visualise time series
pdf(paste(pathToOut,"fig_time_series.pdf",sep=""))
par(mfrow=c(3,4))
for(i in 2:ncol(TS))
{
plot(TS[,1],TS[,i],type="l",xlab="Time step",ylab="Count",bty="n",main=colnames(TS)[i])
}
par(mfrow=c(1,1))
dev.off()
## parameters of observation model
N       = ncol(TS) - 1
K_o     = 10                  # number of ensemble elements
W_o     = rep(30,N)          # number of neurons in observation model, by default a single layer perceptron (equivalent to number of elements in Fourier series)
N_o     = W_o*3              # total number of parameters in observation model
rho     = 1                  # proportion of best samples to reject (to refine quality of fit if necessary)
alpha_i = 1                  # upsampling interpolation factor (2 double the number of points in interpolated time series)
## train observation model
model_o     = trainModel_o(TS,alpha_i,N_o,K_o,rho)
Yhat_o      = model_o$Yhat_o
ddt.Yhat_o  = model_o$ddt.Yhat_o
Omega_o     = model_o$Omega_o
plotModel_o(TS,alpha_i,Yhat_o,ddt.Yhat_o)
save(Yhat_o,    file=paste(pathToOut,"/","Yhat_o.RData"    ,sep=""))
save(ddt.Yhat_o,file=paste(pathToOut,"/","ddt.Yhat_o.RData",sep=""))
save(Omega_o,   file=paste(pathToOut,"/","Omega_o.RData"   ,sep=""))
## load model_o
load(file=paste(pathToOut,"/","Yhat_o.RData"    ,sep=""))
load(file=paste(pathToOut,"/","ddt.Yhat_o.RData",sep=""))
load(file=paste(pathToOut,"/","Omega_o.RData"   ,sep=""))
## parameters of process model
K_p   = 3                                                       # number of models to fit
W_p   = rep(10,N)                                               # number of neurons in single layer perceptron (SLP)
N_p   = 2 * W_p * (2+N)                                         # number of parameters in process model
sd1_p = 0.1                                                     # standard deviation of model likelihood
sd2_p = list(c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2))) # standard deviation of prior distributions (second half concerns nonlinear functions)
## train process model
model_p    = trainModel_p(Yhat_o,ddt.Yhat_o,N_p,sd1_p,sd2_p,K_p)
Yhat_p     = model_p$Yhat_p
ddx.Yhat_p = model_p$ddx.Yhat_p
Geber_p    = model_p$Geber_p
Omega_p    = model_p$Omega_p
plotModel_p(TS,alpha_i,Yhat_p,ddx.Yhat_p,Geber_p)
## store results
save(Yhat_p       ,file=paste(pathToOut,"/","Yhat_p.RData"    ,sep=""))
save(ddx.Yhat_p   ,file=paste(pathToOut,"/","ddx.Yhat_p.RData",sep=""))
save(Geber_p      ,file=paste(pathToOut,"/","Geber_p.RData"   ,sep=""))
save(Omega_p      ,file=paste(pathToOut,"/","Omega_p.RData"   ,sep=""))
## compute Jacobian and contribution matrix
MSq = function(x) mean(x^2)
prop = function(x) x/sum(x)
J = t(matrix(unlist(lapply(ddx.Yhat_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,mean))),ncol=ncol(TS)-1)) ## average across samples then average across time steps
C = t(matrix(unlist(lapply(Geber_p,   function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,MSq))),ncol=ncol(TS)-1)) ## average across samples then take mean square across time steps
C = t(apply(C,1,prop))
## remove effects on surf and bot
J[1:2,] = 0
C[1:2,] = 0
J[,1:2] = 0
C[,1:2] = 0
## thresholding
hist(J)
hist(C)
J = J*(C>0.1)
C = C*(C>0.1)
## visualise
.plot.DIN(J,C,colnames(TS)[-1])
## compute Jacobian and contribution matrix
MSq = function(x) mean(x^2)
prop = function(x) x/sum(x)
J = t(matrix(unlist(lapply(ddx.Yhat_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,mean))),ncol=ncol(TS)-1)) ## average across samples then average across time steps
C = t(matrix(unlist(lapply(Geber_p,   function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,MSq))),ncol=ncol(TS)-1)) ## average across samples then take mean square across time steps
C = t(apply(C,1,prop))
## remove effects on bot
J[1,] = 0
C[1,] = 0
# J[,1] = 0
# C[,1] = 0
## thresholding
hist(J)
hist(C)
J = J*(C>0.1)
C = C*(C>0.1)
## visualise
.plot.DIN(J,C,colnames(TS)[-1])
## compute Jacobian and contribution matrix
MSq = function(x) mean(x^2)
prop = function(x) x/sum(x)
J = t(matrix(unlist(lapply(ddx.Yhat_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,mean))),ncol=ncol(TS)-1)) ## average across samples then average across time steps
C = t(matrix(unlist(lapply(ddx.Yhat_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,MSq))),ncol=ncol(TS)-1)) ## average across samples then take mean square across time steps
C = t(apply(C,1,prop))
## remove effects on bot
J[1,] = 0
C[1,] = 0
# J[,1] = 0
# C[,1] = 0
## thresholding
hist(J)
hist(C)
J = J*(C>0.1)
C = C*(C>0.1)
## visualise
.plot.DIN(J,C,colnames(TS)[-1])
## compute Jacobian and contribution matrix
MSq = function(x) mean(x^2)
prop = function(x) x/sum(x)
J = t(matrix(unlist(lapply(ddx.Yhat_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,mean))),ncol=ncol(TS)-1)) ## average across samples then average across time steps
C = t(matrix(unlist(lapply(ddx.Yhat_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,MSq))),ncol=ncol(TS)-1)) ## average across samples then take mean square across time steps
C = t(apply(C,1,prop))
## remove effects on bot
J[1,] = 0
C[1,] = 0
# J[,1] = 0
# C[,1] = 0
## thresholding
hist(J)
hist(C)
J = J*(C>0.1)
C = C*(C>0.1)
## visualise
.plot.DIN(J,C,colnames(TS)[-1])
## compute Jacobian and contribution matrix
MSq = function(x) mean(x^2)
prop = function(x) x/sum(x)
J = t(matrix(unlist(lapply(ddx.Yhat_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,mean))),ncol=ncol(TS)-1)) ## average across samples then average across time steps
C = t(matrix(unlist(lapply(Geber_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,MSq))),ncol=ncol(TS)-1)) ## average across samples then take mean square across time steps
C = t(apply(C,1,prop))
## remove effects on bot
J[1,] = 0
C[1,] = 0
# J[,1] = 0
# C[,1] = 0
## thresholding
hist(J)
hist(C)
J = J*(C>0.1)
C = C*(C>0.1)
## visualise
.plot.DIN(J,C,colnames(TS)[-1])
## parameters of observation model
N       = ncol(TS) - 1
K_o     = 30                  # number of ensemble elements
W_o     = rep(30,N)          # number of neurons in observation model, by default a single layer perceptron (equivalent to number of elements in Fourier series)
N_o     = W_o*3              # total number of parameters in observation model
rho     = 1                  # proportion of best samples to reject (to refine quality of fit if necessary)
alpha_i = 1                  # upsampling interpolation factor (2 double the number of points in interpolated time series)
## train observation model
model_o     = trainModel_o(TS,alpha_i,N_o,K_o,rho)
Yhat_o      = model_o$Yhat_o
ddt.Yhat_o  = model_o$ddt.Yhat_o
Omega_o     = model_o$Omega_o
## visualise observation model fit
pdf(paste(pathToOut,"/fig_predictions_o.pdf",sep=""))
plotModel_o(TS,alpha_i,Yhat_o,ddt.Yhat_o)
dev.off()
## save results
save(Yhat_o,    file=paste(pathToOut,"/","Yhat_o.RData"    ,sep=""))
save(ddt.Yhat_o,file=paste(pathToOut,"/","ddt.Yhat_o.RData",sep=""))
save(Omega_o,   file=paste(pathToOut,"/","Omega_o.RData"   ,sep=""))
plotModel_o(TS,alpha_i,Yhat_o,ddt.Yhat_o)
## load model_o
load(file=paste(pathToOut,"/","Yhat_o.RData"    ,sep=""))
load(file=paste(pathToOut,"/","ddt.Yhat_o.RData",sep=""))
load(file=paste(pathToOut,"/","Omega_o.RData"   ,sep=""))
## parameters of process model
K_p   = 3                                                       # number of models to fit
W_p   = rep(10,N)                                               # number of neurons in single layer perceptron (SLP)
N_p   = 2 * W_p * (2+N)                                         # number of parameters in process model
sd1_p = 0.1                                                     # standard deviation of model likelihood
sd2_p = list(c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2)),
c(rep(0.03,N_p[1]/2),rep(.03,N_p[1]/2))) # standard deviation of prior distributions (second half concerns nonlinear functions)
## train process model
model_p    = trainModel_p(Yhat_o,ddt.Yhat_o,N_p,sd1_p,sd2_p,K_p)
Yhat_p     = model_p$Yhat_p
ddx.Yhat_p = model_p$ddx.Yhat_p
Geber_p    = model_p$Geber_p
Omega_p    = model_p$Omega_p
plotModel_p(TS,alpha_i,Yhat_p,ddx.Yhat_p,Geber_p)
## store results
save(Yhat_p       ,file=paste(pathToOut,"/","Yhat_p.RData"    ,sep=""))
save(ddx.Yhat_p   ,file=paste(pathToOut,"/","ddx.Yhat_p.RData",sep=""))
save(Geber_p      ,file=paste(pathToOut,"/","Geber_p.RData"   ,sep=""))
save(Omega_p      ,file=paste(pathToOut,"/","Omega_p.RData"   ,sep=""))
## compute Jacobian and contribution matrix
MSq = function(x) mean(x^2)
prop = function(x) x/sum(x)
J = t(matrix(unlist(lapply(ddx.Yhat_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,mean))),ncol=ncol(TS)-1)) ## average across samples then average across time steps
C = t(matrix(unlist(lapply(Geber_p,   function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,MSq))),ncol=ncol(TS)-1)) ## average across samples then take mean square across time steps
C = t(apply(C,1,prop))
## remove effects on bot
J[1,] = 0
C[1,] = 0
J = J*(C>0.1)
C = C*(C>0.1)
## visualise
.plot.DIN(J,C,colnames(TS)[-1])
par(mfrow=c(1,1))
## visualise
.plot.DIN(J,C,colnames(TS)[-1])
par(mfrow=c(3,4))
for(i in 2:ncol(TS))
{
plot(TS[,1],TS[,i],type="l",xlab="Time step",ylab="Count",bty="n",main=colnames(TS)[i])
}
par(mfrow=c(1,1))
dev.off()
par(mfrow=c(3,4))
for(i in 2:ncol(TS))
{
plot(TS[,1],TS[,i],type="l",xlab="Time step",ylab="Count",bty="n",main=colnames(TS)[i])
}
par(mfrow=c(1,1))
## visualise
.plot.DIN(J,C,colnames(TS)[-1])
plotModel_p(TS,alpha_i,Yhat_p,ddx.Yhat_p,Geber_p)
C = t(matrix(unlist(lapply(ddx.Yhar_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,MSq))),ncol=ncol(TS)-1)) ## average across samples then take mean square across time steps
MSq = function(x) mean(x^2)
prop = function(x) x/sum(x)
J = t(matrix(unlist(lapply(ddx.Yhat_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,mean))),ncol=ncol(TS)-1)) ## average across samples then average across time steps
C = t(matrix(unlist(lapply(ddx.Yhat_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,MSq))),ncol=ncol(TS)-1)) ## average across samples then take mean square across time steps
# C = t(matrix(unlist(lapply(Geber_p,   function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,MSq))),ncol=ncol(TS)-1)) ## average across samples then take mean square across time steps
C = t(apply(C,1,prop))
## remove effects on bot
J[1,] = 0
C[1,] = 0
# J[,1] = 0
# C[,1] = 0
## thresholding
hist(J)
hist(C)
J = J*(C>0.1)
C = C*(C>0.1)
## visualise
.plot.DIN(J,C,colnames(TS)[-1])
## compute Jacobian and contribution matrix
MSq = function(x) mean(x^2)
prop = function(x) x/sum(x)
J = t(matrix(unlist(lapply(ddx.Yhat_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,mean))),ncol=ncol(TS)-1)) ## average across samples then average across time steps
C = t(matrix(unlist(lapply(ddx.Yhat_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,MSq))),ncol=ncol(TS)-1)) ## average across samples then take mean square across time steps
# C = t(matrix(unlist(lapply(Geber_p,   function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,MSq))),ncol=ncol(TS)-1)) ## average across samples then take mean square across time steps
C = t(apply(C,1,prop))
## remove effects on bot
J[1,] = 0
C[1,] = 0
# J[,1] = 0
# C[,1] = 0
## thresholding
hist(J)
hist(C)
J = J*(C>0.1)
C = C*(C>0.1)
## visualise
.plot.DIN(J,C,colnames(TS)[-1])
apply(C,2,sum)
apply(C,1,sum)
##
contributor = apply(C,2,sum)
contributed = apply(C,1,sum)
plot(contributor,contributed,col=rainbow(N))
plot(contributor,contributed,col=rainbow(N),pch=16)
plot(contributor[-1],contributed[-1],col=rainbow(N),pch=16)
C
apply(C,1,mean)
##
effector = apply(J,2,mean)
contributor = apply(C,2,mean)
plot(contributor[-1],contributed[-1],col=rainbow(N),pch=16)
plot(effector[-1],contributor[-1],col=rainbow(N),pch=16)
plot(effector[-1],contributor[-1],col=rainbow(N)[-1],pch=16)
N
dev.off()
plot(effector[-1],contributor[-1],col=rainbow(N)[-1],pch=16)
J
plot(effector[-1],contributor[-1],col=rainbow(N,start=0.1,end=0.9)[-1],pch=16)
text(effector[-1],contributor[-1],labels = colnames(TS)[-1])
## visualise
.plot.DIN(J,C,colnames(TS)[-1])
colnames(TS)
plot(effector[-1],contributor[-1],col=rainbow(N,start=0.1,end=0.9)[-1],pch=16)
text(effector[-1],contributor[-1],labels = colnames(TS)[-(1:2)])
N
colnames(TS)
plot(effector[-1],contributor[-1],col=rainbow(N,start=0.1,end=0.9)[-1],pch=16)
## compute Jacobian and contribution matrix
MSq = function(x) mean(x^2)
prop = function(x) x/sum(x)
J = t(matrix(unlist(lapply(ddx.Yhat_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,mean))),ncol=ncol(TS)-1)) ## average across samples then average across time steps
# C = t(matrix(unlist(lapply(ddx.Yhat_p,function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,MSq))),ncol=ncol(TS)-1)) ## average across samples then take mean square across time steps
C = t(matrix(unlist(lapply(Geber_p,   function(x)apply(matrix(apply(x,2,mean),nrow=nrow(TS),byrow=T),2,MSq))),ncol=ncol(TS)-1)) ## average across samples then take mean square across time steps
C = t(apply(C,1,prop))
## remove effects on bot
J[1,] = 0
C[1,] = 0
# J[,1] = 0
# C[,1] = 0
## thresholding
hist(J)
hist(C)
# J = J*(C>0.1)
# C = C*(C>0.1)
## visualise
.plot.DIN(J,C,colnames(TS)[-1])
effector = apply(J,2,mean)
contributor = apply(C,2,mean)
plot(effector[-1],contributor[-1],col=rainbow(N,start=0.1,end=0.9)[-1],pch=16)
effector = apply(J,2,mean)
contributor = apply(C,2,mean)
plot(effector,contributor,col=rainbow(N,start=0.1,end=0.9)[-1],pch=16)
N
plot(effector,contributor,col=rainbow(N,start=0.1,end=0.9),pch=16)
plot(effector,contributor,col=rainbow(N,start=0.1,end=0.9)[-1],pch=16)
ncol(TS)
text(effector[-1],contributor[-1],labels = colnames(TS)[-(1:2)])
text(effector[-1],contributor[-1],labels = colnames(TS)[-1])
plot(effector,contributor,col=rainbow(N,start=0.1,end=0.9)[-1],pch=16)
text(effector[-1],contributor[-1],labels = colnames(TS)[-1])
plot(effector,contributor,col=rainbow(N,start=0.1,end=0.9)[-1],pch=16)
text(effector,contributor,labels = colnames(TS)[-1])
effector = apply(J,2,mean)
contributor = apply(C,2,mean)
plot(effector,log(contributor),col=rainbow(N,start=0.1,end=0.9)[-1],pch=16)
text(effector,log(contributor),labels = colnames(TS)[-1])
crossVal_p = function(TS,alpha_i,Yhat_o,ddt.Yhat_o,N_p,sd1_p,sd2_p,K_p,folds,crossValParVect)
{
## load data
attach(loadData_o(TS,alpha_i),warn.conflicts=F)
attach(loadData_p(Yhat_o,ddt.Yhat_o),warn.conflicts=F)
## data specs
N   = length(Yhat_o)
n   = ncol(Yhat_o[[1]])
## for each variable
Omega_p    = list()
crossVal_p = list()
for(i in 1:N)
{
## iterator
message(paste("fitting: ",i,"/",N,sep=""))
## cross validation
crossVal_i   = NULL
Omega_p[[i]] = list()
for(k in 1:length(crossValParVect)) # for each regularisation param
{
## iterator
message(paste("crossval: ",k,"/",length(crossValParVect),sep=""))
## multiple folds
crossVal_ik          = NULL
Omega_p[[i]][[k]]    = list()
for(u in 1:length(folds)) # for each fold in the data
{
## iterator
message(paste("fold: ",u,"/",length(folds),sep=""))
## fit model
Omega_p_iku = NULL
for(m in 1:K_p)
{
## dataloader
attach(loadData_o(TS,alpha_i),warn.conflicts=F)
## split training/test
s_l   = round(folds[[u]][1]*n+1):round(folds[[u]][2]*n)
s_t   = round(folds[[u]][3]*n+1):round(folds[[u]][4]*n)
X_l = X_[s_l,]
Y_l = Y_[s_l,]
X_t = X_[s_t,]
Y_t = Y_[s_t,]
## TO MODULARISE ##
sd2_p[[i]] = crossValParVect[k] # for linear and nonlinear part of network
# sd2_p[[i]][(N_p[i]/2):N_p[i]] = crossValParVect[k] # for nonlinear part of network only
## fit
Omega_0      = rnorm(N_p[i],0,0.001)
Yhat         = function(X,Omega) f_p.eval(X,Omega)
ddOmega.Yhat = function(X,Omega) ddOmega.f_p.eval(X,Omega)
# Omega_f      = argmax.logMarPost(X_l,Y_l[,i],Yhat,ddOmega.Yhat,Omega_0,1/W_p[i])
Omega_f      = argmax.logPost(X_l,Y_l[,i],Yhat,ddOmega.Yhat,Omega_0,sd1_p,sd2_p[[i]])
Omega_p_iku  = rbind(Omega_p_iku,Omega_f)
## update
logPost_0 = logPost(X_l,Y_l[,i],Yhat,Omega_0,sd1_p,sd2_p[[i]])
logPost_f = logPost(X_l,Y_l[,i],Yhat,Omega_f,sd1_p,sd2_p[[i]])
## model performance
logLik_l = logLik(X_l,Y_l[,i],Yhat,Omega_f,sd1_p)
logLik_t = logLik(X_t,Y_t[,i],Yhat,Omega_f,sd1_p)
## cross validation matrix
crossVal_ik  = rbind(crossVal_ik,cbind(logLik_l,logLik_t))
}
Omega_p[[i]][[k]][[u]] = Omega_p_iku
}
## store
E.crossVal_ik  = apply(crossVal_ik,2,mean)
sd.crossVal_ik = apply(crossVal_ik,2,sd)
crossVal_i     = rbind(crossVal_i,c(crossValParVect[k],E.crossVal_ik,sd.crossVal_ik))
message(paste("logLik l vs t: ",
format(round(E.crossVal_ik[1],2),nsmall=2),"\t",
format(round(E.crossVal_ik[2],2),nsmall=2),"\n",sep=""))
}
message("\n")
## store
crossVal_p[[i]] = crossVal_i
colnames(crossVal_p[[i]]) = c("sd","logLik_l","logLik_t","sd.logLik_l","sd.logLik_t")
# colnames(crossVal_p[[i]]) = c("w","logMarLik_l","logMarLik_t","sd.logMarLik_l","sd.logMarLik_t")
}
message("\n")
return(crossVal_p)
}
## load model_o
load(file=paste(pathToOut,"/","Yhat_o.RData"    ,sep=""))
load(file=paste(pathToOut,"/","ddt.Yhat_o.RData",sep=""))
load(file=paste(pathToOut,"/","Omega_o.RData"   ,sep=""))
## parameters for cross-validation
K_p             = 3                                      # number of models to fit per folds and regularisation parameter
folds           = list(c(1/4,2/4,2/4,3/4))               # proportion of the data that should be considered for training
crossValParVect = seq(0.005,0.05,0.005)
## run cross-validation
resultsCrossVal_p = crossVal_p(TS,alpha_i,Yhat_o,ddt.Yhat_o,N_p,sd1_p,sd2_p,K_p,folds,crossValParVect)
plotCrossVal_p(resultsCrossVal_p)
## visualise
pdf(paste(pathToOut,"/fig_crossVal_p.pdf",sep=""))
plotCrossVal_p(resultsCrossVal_p)
dev.off()
## store results
save(resultsCrossVal_p,file=paste(pathToOut,"/","crossVal_p.RData"   ,sep=""))
